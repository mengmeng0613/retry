import jieba
import requests
import streamlit as st
from streamlit_echarts import st_echarts
from collections import Counter
from bs4 import BeautifulSoup
import re
import string
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import os

# æ¸…ç†å’Œé¢„å¤„ç†æ–‡æœ¬å‡½æ•°
def preprocess_text(text):
    text = re.sub(r'\s+', '', text)  # å»é™¤ç©ºç™½å­—ç¬¦
    text = re.sub(r'[\n\r]', '', text)  # å»é™¤æ¢è¡Œç¬¦
    text = re.sub(r'[^\w\s]', '', text)  # å»é™¤æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'\d+', '', text)  # å»é™¤æ•°å­—
    return text.strip()

# åˆ†è¯å‡½æ•°
def word_segmentation(text):
    stopwords = set(
        ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'ä¹‹', 'ä¸', 'å’Œ', 'æˆ–', 'è™½ç„¶', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å› æ­¤', 'æ—¥', 'æœˆ', 'è½¬å‘', 'æ”¶è—', 'å–æ¶ˆ', 'ç±»', 'å¹´', 'è¯·', 'å¾®ä¿¡', 'å…¶ä»–'])
    words = jieba.lcut(text)
    return [word for word in words if word not in stopwords]

# æå–æ­£æ–‡æ–‡æœ¬
def extract_main_text(html):
    soup = BeautifulSoup(html, 'html.parser')
    content = soup.select('.search-result-item')  # ä¿®æ”¹é€‰æ‹©å™¨ä»¥åŒ¹é…å®é™…é¡µé¢ç»“æ„
    if content:
        return ' '.join([c.get_text() for c in content])
    return soup.get_text()

# ç”Ÿæˆè¯äº‘å›¾
def generate_wordcloud(word_counts):
    if word_counts:
        font_path = os.path.join(os.path.dirname(__file__), 'simhei.ttf')
        if not os.path.exists(font_path):
            st.error(f"å­—ä½“æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{font_path}")
            return

        try:
            wordcloud = WordCloud(font_path=font_path, width=800, height=400).generate_from_frequencies(word_counts)
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            st.pyplot(plt)
        except Exception as e:
            st.error(f"ç”Ÿæˆè¯äº‘å›¾æ—¶å‡ºç°é”™è¯¯: {e}")
    else:
        st.write("æ²¡æœ‰è¶³å¤Ÿçš„è¯è¯­ç”Ÿæˆè¯äº‘å›¾ã€‚")

# è¿è¡Œä¸»ç¨‹åº
def main():
    st.set_page_config(
        page_title="æ–‡æœ¬å¤„ç†",
        page_icon="ğŸ“",
    )

    st.title("æ¬¢è¿ä½¿ç”¨ Streamlit æ–‡æœ¬å¤„ç† ğŸ“")

    base_url = st.text_input('è¯·è¾“å…¥åŸºç¡€ URL :')
    num_pages = st.number_input('è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°:', min_value=1, value=20)

    if base_url:
        all_text = ""

        for page in range(1, num_pages + 1):
            url = f"{base_url}&page={page}"
            try:
                response = requests.get(url)
                response.encoding = 'utf-8'
                html_content = response.text

                st.write(f"è·å–ç¬¬ {page} é¡µå†…å®¹æˆåŠŸ")

                text = extract_main_text(html_content)
                st.text_area(f"ç¬¬ {page} é¡µæå–çš„æ­£æ–‡æ–‡æœ¬ï¼š", text, height=200)
                all_text += text

            except Exception as e:
                st.error(f"çˆ¬å–ç¬¬ {page} é¡µæ—¶å‡ºç°é”™è¯¯: {e}")

        if all_text:
            st.write("æ‰€æœ‰é¡µå†…å®¹åˆå¹¶æˆåŠŸ")

            # é¢„å¤„ç†æ–‡æœ¬
            text_preprocessed = preprocess_text(all_text)
            st.text_area("é¢„å¤„ç†åçš„æ–‡æœ¬ï¼š", text_preprocessed[:500], height=200)

            words = word_segmentation(text_preprocessed)
            st.write("åˆ†è¯ç»“æœï¼š", words[:50])  # ä»…å±•ç¤ºå‰50ä¸ªè¯

            word_count = Counter(words)
            most_common_words = word_count.most_common(20)

            st.write("è¯é¢‘ç»Ÿè®¡ç»“æœï¼š", most_common_words)

            if most_common_words:
                chart_options = {
                    "tooltip": {"trigger": 'item', "formatter": '{b} : {c}'},
                    "xAxis": [{
                        "type": "category",
                        "data": [word for word, count in most_common_words],
                        "axisLabel": {"interval": 0, "rotate": 45}
                    }],
                    "yAxis": [{"type": "value"}],
                    "series": [{
                        "type": "bar",
                        "data": [count for word, count in most_common_words]
                    }]
                }

                st_echarts(chart_options, height='500px')

                # ç”Ÿæˆè¯äº‘å›¾
                st.write("è¯äº‘å›¾ï¼š")
                generate_wordcloud(dict(most_common_words))
            else:
                st.write("æ²¡æœ‰è¶³å¤Ÿçš„è¯è¯­ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚")

if __name__ == "__main__":
    main()
